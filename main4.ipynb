{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accent-recognizing model, same as in main2.ipynb\n",
    "- Including Yamnet model.\n",
    "\n",
    "Not included:\n",
    "- Including hyperparam tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q tensorflow tensorflow_datasets\n",
    "#apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "%pip install -U -q keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from os.path import isfile, join, splitext\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_hub as tfhub\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "import keras_tuner as kt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_SEED = 43\n",
    "MODEL_NAME = \"uu_accent_recognition\"\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many classes are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 228 classes.\n",
      "There are 228 classes.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/data/audio.csv\", on_bad_lines = 'skip' , delimiter= ';')\n",
    "# print(len(df['native_langs'].unique()))\n",
    "# for lang in df['native_langs'].unique():\n",
    "#     print(f'\"{lang}\",')\n",
    "class_names = df['native_langs'].unique()\n",
    "lang_idxs = range(len(class_names))\n",
    "class_dict = dict(zip(class_names, lang_idxs))\n",
    "\n",
    "print(f\"There are {len(class_names)} classes.\")\n",
    "print(f\"There are {len(class_dict)} classes.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "\n",
    "**WARNING** -- This method of loading the audio files does not work with the Yamnet model. A different approach will be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_audio(path):\n",
    "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "    return audio\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    \n",
    "    # print(dataframe.info())\n",
    "    # Rewrite file_name to contain file paths\n",
    "    dataframe['file_name'] = dataframe.apply(\n",
    "        lambda row: os.path.join(os.getcwd(), 'data/audio_wav', row[\"file_name\"] + \".wav\"), \n",
    "        axis=1\n",
    "    )\n",
    "    # Convert the labels into numbers\n",
    "    dataframe['native_langs'] = dataframe.apply(\n",
    "        lambda row: class_dict[row['native_langs']],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(dataframe['file_name'])\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(dataframe['native_langs'])\n",
    "    \n",
    "    # print(len(path_ds), len(audio_ds), len(label_ds))\n",
    "    # dataframe = df[(df[\"native_langs\"] == \"amharic\") | (df[\"native_langs\"] == \"indonesian\")]\n",
    "    # print(dataframe.shape)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def audio_to_fft(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension, we need to squeeze the dimensions and then expand them again after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the list of audio file paths and labels from the CSV file, as Pandas dataframe.\n",
    "\n",
    "**IMPORTANT!**\n",
    "To avoid noises, we are experimenting with only the most frequent classes in our dataset (English and Spanish). In order to work with the entire classes, we are going to identify infrequent classes and remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(os.getcwd() + \"/data/audio.csv\", on_bad_lines = 'skip' , delimiter= ';')\n",
    "dataframe = dataframe[(dataframe['native_langs'] == 'english') | (dataframe['native_langs'] == 'spanish')]\n",
    "print(len(dataframe))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training & validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 21:24:12.479185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "# rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "# rng.shuffle(audio_paths)\n",
    "# rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "# rng.shuffle(labels)\n",
    "\n",
    "# Splitting training and validation set\n",
    "split = int(len(dataframe) * 0.8)\n",
    "train_df = dataframe[:split]\n",
    "valid_df = dataframe[split:]\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_df)\n",
    "valid_ds = dataframe_to_dataset(valid_df)\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(BATCH_SIZE)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yamnet Model\n",
    "\n",
    "#### Prepare Dataset\n",
    "\n",
    "Yamnet requires a slightly different way of preprocessing the dataset. Note that we **do not do data shuffling** as before. \n",
    "\n",
    "TODO: Need to see if this affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model = tf.saved_model.load('./yamnet_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_wav = path_to_audio('data/audio_wav/afrikaans1.wav')\n",
    "\n",
    "def load_16k_audio_wav(filename):\n",
    "    # Read file content\n",
    "    file_content = tf.io.read_file(filename)\n",
    "\n",
    "    # Decode audio wave\n",
    "    audio_wav, sample_rate = tf.audio.decode_wav(file_content, desired_channels=1)\n",
    "    audio_wav = tf.squeeze(audio_wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "\n",
    "    # Resample to 16k\n",
    "    audio_wav = tfio.audio.resample(audio_wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return audio_wav\n",
    "\n",
    "\n",
    "def filepath_to_embeddings(filename, label):\n",
    "    # Load 16k audio wave\n",
    "    audio_wav = load_16k_audio_wav(filename)\n",
    "\n",
    "    # Get audio embeddings & scores.\n",
    "    # The embeddings are the audio features extracted using transfer learning\n",
    "    # while scores will be used to identify time slots that are not speech\n",
    "    # which will then be gathered into a specific new category 'other'\n",
    "    scores, embeddings, _ = yamnet_model(audio_wav)\n",
    "\n",
    "    # Number of embeddings in order to know how many times to repeat the label\n",
    "    embeddings_num = tf.shape(embeddings)[0]\n",
    "    labels = tf.repeat(label, embeddings_num)\n",
    "\n",
    "    # Change labels for time-slots that are not speech into a new category 'other'\n",
    "    # labels = tf.where(tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1)\n",
    "\n",
    "    # Using one-hot in order to use AUC\n",
    "    return (embeddings, tf.one_hot(labels, len(class_names)))\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(dataframe, batch_size=64):\n",
    "    # print(dataframe.info())\n",
    "    # Rewrite file_name to contain file paths\n",
    "    dataframe['file_name'] = dataframe.apply(\n",
    "        lambda row: os.path.join(os.getcwd(), 'data/audio_wav', row[\"file_name\"] + \".wav\"), \n",
    "        axis=1\n",
    "    )\n",
    "    # Convert the labels into numbers\n",
    "    dataframe['native_langs'] = dataframe.apply(\n",
    "        lambda row: class_dict[row['native_langs']],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"file_name\"], dataframe[\"native_langs\"]))\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: filepath_to_embeddings(x, y),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    ).unbatch()\n",
    "\n",
    "    return dataset.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 886\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(os.getcwd() + \"/data/audio.csv\", on_bad_lines = 'skip' , delimiter= ';')\n",
    "dataframe = dataframe[(dataframe['native_langs'] == 'english') | (dataframe['native_langs'] == 'spanish')]\n",
    "print(f'Number of data points: {len(dataframe)}')\n",
    "\n",
    "# Splitting training and validation set\n",
    "split = int(len(dataframe) * 0.8)\n",
    "train_df = dataframe[:split]\n",
    "valid_df = dataframe[split:]\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_df)\n",
    "valid_ds = dataframe_to_dataset(valid_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"accent_recognition\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (InputLayer)      [(None, 1024)]            0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 384)               98688     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 192)               73920     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 384)               74112     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 384)               0         \n",
      "                                                                 \n",
      " ouput (Dense)               (None, 228)               87780     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,900\n",
      "Trainable params: 596,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_and_compile_model():\n",
    "    inputs = keras.layers.Input(shape=(1024), name=\"embedding\")\n",
    "\n",
    "    x = keras.layers.Dense(256, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = keras.layers.Dropout(0.15, name=\"dropout_1\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    x = keras.layers.Dropout(0.2, name=\"dropout_2\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(192, activation=\"relu\", name=\"dense_3\")(x)\n",
    "    x = keras.layers.Dropout(0.25, name=\"dropout_3\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_4\")(x)\n",
    "    x = keras.layers.Dropout(0.2, name=\"dropout_4\")(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", name=\"ouput\")(\n",
    "        x\n",
    "    )\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"accent_recognition\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1.9644e-5),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_and_compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class weight calculation\n",
    "\n",
    "Since the dataset is quite unbalanced, we wil use class_weight argument during training.\n",
    "\n",
    "Getting the class weights is a little tricky because even though we know the number of audio files for each class, it does not represent the number of samples for that class since Yamnet transforms each audio file into multiple audio samples of 0.96 seconds each. So every audio file will be split into a number of samples that is proportional to its length.\n",
    "\n",
    "Therefore, to get those weights, we have to calculate the number of samples for each class after preprocessing through Yamnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: inf, 1: inf, 2: inf, 3: inf, 4: inf, 5: inf, 6: inf, 7: inf, 8: inf, 9: inf, 10: inf, 11: inf, 12: inf, 13: inf, 14: inf, 15: inf, 16: inf, 17: inf, 18: inf, 19: inf, 20: inf, 21: inf, 22: inf, 23: inf, 24: inf, 25: inf, 26: 1.3341108310447312, 27: inf, 28: inf, 29: inf, 30: inf, 31: inf, 32: inf, 33: inf, 34: inf, 35: inf, 36: inf, 37: inf, 38: inf, 39: inf, 40: inf, 41: inf, 42: inf, 43: inf, 44: inf, 45: inf, 46: inf, 47: inf, 48: inf, 49: inf, 50: inf, 51: inf, 52: inf, 53: inf, 54: inf, 55: inf, 56: inf, 57: inf, 58: inf, 59: inf, 60: inf, 61: inf, 62: inf, 63: inf, 64: inf, 65: inf, 66: inf, 67: inf, 68: inf, 69: inf, 70: inf, 71: inf, 72: inf, 73: inf, 74: inf, 75: inf, 76: inf, 77: inf, 78: inf, 79: inf, 80: inf, 81: inf, 82: inf, 83: inf, 84: inf, 85: inf, 86: inf, 87: inf, 88: 3.9930188041887176, 89: inf, 90: inf, 91: inf, 92: inf, 93: inf, 94: inf, 95: inf, 96: inf, 97: inf, 98: inf, 99: inf, 100: inf, 101: inf, 102: inf, 103: inf, 104: inf, 105: inf, 106: inf, 107: inf, 108: inf, 109: inf, 110: inf, 111: inf, 112: inf, 113: inf, 114: inf, 115: inf, 116: inf, 117: inf, 118: inf, 119: inf, 120: inf, 121: inf, 122: inf, 123: inf, 124: inf, 125: inf, 126: inf, 127: inf, 128: inf, 129: inf, 130: inf, 131: inf, 132: inf, 133: inf, 134: inf, 135: inf, 136: inf, 137: inf, 138: inf, 139: inf, 140: inf, 141: inf, 142: inf, 143: inf, 144: inf, 145: inf, 146: inf, 147: inf, 148: inf, 149: inf, 150: inf, 151: inf, 152: inf, 153: inf, 154: inf, 155: inf, 156: inf, 157: inf, 158: inf, 159: inf, 160: inf, 161: inf, 162: inf, 163: inf, 164: inf, 165: inf, 166: inf, 167: inf, 168: inf, 169: inf, 170: inf, 171: inf, 172: inf, 173: inf, 174: inf, 175: inf, 176: inf, 177: inf, 178: inf, 179: inf, 180: inf, 181: inf, 182: inf, 183: inf, 184: inf, 185: inf, 186: inf, 187: inf, 188: inf, 189: inf, 190: inf, 191: inf, 192: inf, 193: inf, 194: inf, 195: inf, 196: inf, 197: inf, 198: inf, 199: inf, 200: inf, 201: inf, 202: inf, 203: inf, 204: inf, 205: inf, 206: inf, 207: inf, 208: inf, 209: inf, 210: inf, 211: inf, 212: inf, 213: inf, 214: inf, 215: inf, 216: inf, 217: inf, 218: inf, 219: inf, 220: inf, 221: inf, 222: inf, 223: inf, 224: inf, 225: inf, 226: inf, 227: inf}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3p/fc9cp69s1lsfgnzqtqtddvz00000gn/T/ipykernel_15332/1312827400.py:9: RuntimeWarning: divide by zero encountered in int_scalars\n",
      "  i: tf.math.reduce_sum(class_counts).numpy() / class_counts[i].numpy()\n"
     ]
    }
   ],
   "source": [
    "class_counts = tf.zeros(shape=(len(class_names),), dtype=tf.int32)\n",
    "\n",
    "for x, y in iter(train_ds):\n",
    "    class_counts = class_counts + tf.math.bincount(\n",
    "        tf.cast(tf.math.argmax(y, axis=1), tf.int32), minlength=len(class_names)\n",
    "    )\n",
    "\n",
    "class_weight = {\n",
    "    i: tf.math.reduce_sum(class_counts).numpy() / class_counts[i].numpy()\n",
    "    for i in range(len(class_counts))\n",
    "}\n",
    "\n",
    "print(class_weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks\n",
    "We use Keras callbacks in order to:\n",
    "\n",
    "- Stop whenever the validation AUC stops improving.\n",
    "- Save the best model.\n",
    "- Call TensorBoard in order to later view the training and validation logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\", patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    MODEL_NAME + \".h5\", monitor=\"val_auc\", save_best_only=True\n",
    ")\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "    os.path.join(os.curdir, \"logs\", model.name)\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "555/555 - 59s - loss: 4.5469 - accuracy: 0.5902 - auc: 0.9566 - val_loss: 0.7492 - val_accuracy: 0.5868 - val_auc: 0.9974 - 59s/epoch - 106ms/step\n",
      "Epoch 2/50\n",
      "555/555 - 8s - loss: 1.6634 - accuracy: 0.5295 - auc: 0.9978 - val_loss: 0.6929 - val_accuracy: 0.5868 - val_auc: 0.9978 - 8s/epoch - 14ms/step\n",
      "Epoch 3/50\n",
      "555/555 - 8s - loss: 1.5572 - accuracy: 0.5191 - auc: 0.9979 - val_loss: 0.6767 - val_accuracy: 0.5868 - val_auc: 0.9985 - 8s/epoch - 14ms/step\n",
      "Epoch 4/50\n",
      "555/555 - 8s - loss: 1.5053 - accuracy: 0.5181 - auc: 0.9979 - val_loss: 0.6686 - val_accuracy: 0.5873 - val_auc: 0.9985 - 8s/epoch - 14ms/step\n",
      "Epoch 5/50\n",
      "555/555 - 9s - loss: 1.4795 - accuracy: 0.5235 - auc: 0.9979 - val_loss: 0.6639 - val_accuracy: 0.5886 - val_auc: 0.9985 - 9s/epoch - 15ms/step\n",
      "Epoch 6/50\n",
      "555/555 - 8s - loss: 1.4612 - accuracy: 0.5265 - auc: 0.9980 - val_loss: 0.6601 - val_accuracy: 0.5910 - val_auc: 0.9985 - 8s/epoch - 14ms/step\n",
      "Epoch 7/50\n",
      "555/555 - 9s - loss: 1.4471 - accuracy: 0.5272 - auc: 0.9980 - val_loss: 0.6566 - val_accuracy: 0.5978 - val_auc: 0.9985 - 9s/epoch - 16ms/step\n",
      "Epoch 8/50\n",
      "555/555 - 9s - loss: 1.4346 - accuracy: 0.5323 - auc: 0.9980 - val_loss: 0.6547 - val_accuracy: 0.5969 - val_auc: 0.9985 - 9s/epoch - 16ms/step\n",
      "Epoch 9/50\n",
      "555/555 - 8s - loss: 1.4263 - accuracy: 0.5323 - auc: 0.9980 - val_loss: 0.6531 - val_accuracy: 0.6039 - val_auc: 0.9985 - 8s/epoch - 14ms/step\n",
      "Epoch 10/50\n",
      "555/555 - 9s - loss: 1.4130 - accuracy: 0.5422 - auc: 0.9980 - val_loss: 0.6513 - val_accuracy: 0.6120 - val_auc: 0.9986 - 9s/epoch - 15ms/step\n",
      "Epoch 11/50\n",
      "555/555 - 8s - loss: 1.4039 - accuracy: 0.5460 - auc: 0.9981 - val_loss: 0.6495 - val_accuracy: 0.6171 - val_auc: 0.9986 - 8s/epoch - 15ms/step\n",
      "Epoch 12/50\n",
      "555/555 - 10s - loss: 1.3963 - accuracy: 0.5496 - auc: 0.9981 - val_loss: 0.6482 - val_accuracy: 0.6236 - val_auc: 0.9986 - 10s/epoch - 18ms/step\n",
      "Epoch 13/50\n",
      "555/555 - 8s - loss: 1.3893 - accuracy: 0.5566 - auc: 0.9982 - val_loss: 0.6466 - val_accuracy: 0.6268 - val_auc: 0.9986 - 8s/epoch - 15ms/step\n",
      "Epoch 14/50\n",
      "555/555 - 9s - loss: 1.3831 - accuracy: 0.5636 - auc: 0.9982 - val_loss: 0.6455 - val_accuracy: 0.6283 - val_auc: 0.9986 - 9s/epoch - 16ms/step\n",
      "Epoch 15/50\n",
      "555/555 - 7s - loss: 1.3758 - accuracy: 0.5695 - auc: 0.9982 - val_loss: 0.6446 - val_accuracy: 0.6343 - val_auc: 0.9986 - 7s/epoch - 13ms/step\n",
      "Epoch 16/50\n",
      "555/555 - 7s - loss: 1.3720 - accuracy: 0.5725 - auc: 0.9982 - val_loss: 0.6444 - val_accuracy: 0.6363 - val_auc: 0.9986 - 7s/epoch - 13ms/step\n",
      "Epoch 17/50\n",
      "555/555 - 8s - loss: 1.3693 - accuracy: 0.5777 - auc: 0.9983 - val_loss: 0.6442 - val_accuracy: 0.6400 - val_auc: 0.9987 - 8s/epoch - 15ms/step\n",
      "Epoch 18/50\n",
      "555/555 - 8s - loss: 1.3622 - accuracy: 0.5830 - auc: 0.9983 - val_loss: 0.6440 - val_accuracy: 0.6435 - val_auc: 0.9987 - 8s/epoch - 14ms/step\n",
      "Epoch 19/50\n",
      "555/555 - 8s - loss: 1.3593 - accuracy: 0.5836 - auc: 0.9983 - val_loss: 0.6430 - val_accuracy: 0.6435 - val_auc: 0.9987 - 8s/epoch - 15ms/step\n",
      "Epoch 20/50\n",
      "555/555 - 9s - loss: 1.3548 - accuracy: 0.5901 - auc: 0.9983 - val_loss: 0.6421 - val_accuracy: 0.6489 - val_auc: 0.9987 - 9s/epoch - 15ms/step\n",
      "Epoch 21/50\n",
      "555/555 - 10s - loss: 1.3512 - accuracy: 0.5925 - auc: 0.9984 - val_loss: 0.6416 - val_accuracy: 0.6520 - val_auc: 0.9987 - 10s/epoch - 17ms/step\n",
      "Epoch 22/50\n",
      "555/555 - 10s - loss: 1.3490 - accuracy: 0.5965 - auc: 0.9984 - val_loss: 0.6418 - val_accuracy: 0.6533 - val_auc: 0.9987 - 10s/epoch - 18ms/step\n",
      "Epoch 23/50\n",
      "555/555 - 9s - loss: 1.3452 - accuracy: 0.6006 - auc: 0.9984 - val_loss: 0.6418 - val_accuracy: 0.6514 - val_auc: 0.9987 - 9s/epoch - 16ms/step\n",
      "Epoch 24/50\n",
      "555/555 - 10s - loss: 1.3414 - accuracy: 0.6038 - auc: 0.9984 - val_loss: 0.6413 - val_accuracy: 0.6508 - val_auc: 0.9987 - 10s/epoch - 19ms/step\n",
      "Epoch 25/50\n",
      "555/555 - 7s - loss: 1.3388 - accuracy: 0.6040 - auc: 0.9984 - val_loss: 0.6409 - val_accuracy: 0.6534 - val_auc: 0.9987 - 7s/epoch - 13ms/step\n",
      "Epoch 26/50\n",
      "555/555 - 8s - loss: 1.3375 - accuracy: 0.6070 - auc: 0.9984 - val_loss: 0.6408 - val_accuracy: 0.6553 - val_auc: 0.9987 - 8s/epoch - 14ms/step\n",
      "Epoch 27/50\n",
      "555/555 - 7s - loss: 1.3329 - accuracy: 0.6061 - auc: 0.9984 - val_loss: 0.6399 - val_accuracy: 0.6546 - val_auc: 0.9987 - 7s/epoch - 13ms/step\n",
      "Epoch 28/50\n",
      "555/555 - 7s - loss: 1.3292 - accuracy: 0.6115 - auc: 0.9985 - val_loss: 0.6402 - val_accuracy: 0.6561 - val_auc: 0.9987 - 7s/epoch - 13ms/step\n",
      "Epoch 29/50\n",
      "555/555 - 9s - loss: 1.3287 - accuracy: 0.6131 - auc: 0.9985 - val_loss: 0.6402 - val_accuracy: 0.6559 - val_auc: 0.9987 - 9s/epoch - 16ms/step\n",
      "Epoch 30/50\n",
      "555/555 - 9s - loss: 1.3245 - accuracy: 0.6157 - auc: 0.9985 - val_loss: 0.6398 - val_accuracy: 0.6574 - val_auc: 0.9987 - 9s/epoch - 17ms/step\n",
      "Epoch 31/50\n",
      "555/555 - 7s - loss: 1.3249 - accuracy: 0.6137 - auc: 0.9985 - val_loss: 0.6399 - val_accuracy: 0.6562 - val_auc: 0.9987 - 7s/epoch - 13ms/step\n",
      "Epoch 32/50\n",
      "555/555 - 7s - loss: 1.3217 - accuracy: 0.6127 - auc: 0.9985 - val_loss: 0.6392 - val_accuracy: 0.6572 - val_auc: 0.9987 - 7s/epoch - 13ms/step\n",
      "Epoch 33/50\n",
      "555/555 - 8s - loss: 1.3210 - accuracy: 0.6159 - auc: 0.9985 - val_loss: 0.6393 - val_accuracy: 0.6575 - val_auc: 0.9987 - 8s/epoch - 14ms/step\n",
      "Epoch 34/50\n",
      "555/555 - 9s - loss: 1.3162 - accuracy: 0.6199 - auc: 0.9985 - val_loss: 0.6391 - val_accuracy: 0.6567 - val_auc: 0.9987 - 9s/epoch - 16ms/step\n",
      "Epoch 35/50\n",
      "555/555 - 8s - loss: 1.3142 - accuracy: 0.6202 - auc: 0.9985 - val_loss: 0.6388 - val_accuracy: 0.6562 - val_auc: 0.9987 - 8s/epoch - 15ms/step\n",
      "Epoch 36/50\n",
      "555/555 - 8s - loss: 1.3147 - accuracy: 0.6177 - auc: 0.9985 - val_loss: 0.6388 - val_accuracy: 0.6550 - val_auc: 0.9987 - 8s/epoch - 14ms/step\n",
      "Epoch 37/50\n",
      "555/555 - 10s - loss: 1.3112 - accuracy: 0.6198 - auc: 0.9985 - val_loss: 0.6390 - val_accuracy: 0.6567 - val_auc: 0.9987 - 10s/epoch - 18ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Plotting the accuracy and AUC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "axs[0].plot(range(EPOCHS), history.history[\"accuracy\"], label=\"Training\")\n",
    "axs[0].plot(range(EPOCHS), history.history[\"val_accuracy\"], label=\"Validation\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_title(\"Training & Validation Accuracy\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(range(EPOCHS), history.history[\"auc\"], label=\"Training\")\n",
    "axs[1].plot(range(EPOCHS), history.history[\"val_auc\"], label=\"Validation\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_title(\"Training & Validation AUC\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555/555 [==============================] - 3s 5ms/step - loss: 0.7286 - accuracy: 0.4823 - auc: 0.9977\n",
      "164/164 [==============================] - 1s 5ms/step - loss: 0.6396 - accuracy: 0.6573 - auc: 0.9987\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc, train_auc = model.evaluate(train_ds)\n",
    "valid_loss, valid_acc, valid_auc = model.evaluate(valid_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Let's now plot the confusion matrix for the validation dataset.\n",
    "\n",
    "The confusion matrix lets us see, for every class, not only how many samples were correctly classified, but also which other classes were the samples confused with.\n",
    "\n",
    "It allows us to calculate the precision and recall for every class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y tensors\n",
    "x_valid = None\n",
    "y_valid = None\n",
    "\n",
    "for x, y in iter(valid_ds):\n",
    "    if x_valid is None:\n",
    "        x_valid = x.numpy()\n",
    "        y_valid = y.numpy()\n",
    "    else:\n",
    "        x_valid = np.concatenate((x_valid, x.numpy()), axis=0)\n",
    "        y_valid = np.concatenate((y_valid, y.numpy()), axis=0)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(x_valid)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(\n",
    "    np.argmax(y_valid, axis=1), np.argmax(y_pred, axis=1)\n",
    ")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    confusion_mtx, xticklabels=class_names, yticklabels=class_names, annot=True, fmt=\"g\"\n",
    ")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision & Recall\n",
    "\n",
    "For every class:\n",
    "\n",
    "- Recall is the ratio of correctly classified samples i.e. it shows how many samples of this specific class, the model is able to detect. It is the ratio of diagonal elements to the sum of all elements in the row.\n",
    "- Precision shows the accuracy of the classifier. It is the ratio of correctly predicted samples among the ones classified as belonging to this class. It is the ratio of diagonal elements to the sum of all elements in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(class_names):\n",
    "    precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])\n",
    "    recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])\n",
    "    print(\n",
    "        \"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(\n",
    "            label, precision * 100, recall * 100\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
