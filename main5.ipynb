{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accent-recognizing model, same as in main2.ipynb\n",
    "- Including Yamnet model.\n",
    "- Including hyperparam tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q tensorflow tensorflow_datasets\n",
    "#apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "%pip install -U -q keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "2023-01-19 18:02:13.007373: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from os.path import isfile, join, splitext\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_hub as tfhub\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "import keras_tuner as kt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_SEED = 43\n",
    "MODEL_NAME = \"uu_accent_recognition\"\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 228 classes.\n",
      "There are 228 classes.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/data/audio.csv\", on_bad_lines = 'skip' , delimiter= ';')\n",
    "# print(len(df['native_langs'].unique()))\n",
    "# for lang in df['native_langs'].unique():\n",
    "#     print(f'\"{lang}\",')\n",
    "class_names = df['native_langs'].unique()\n",
    "lang_idxs = range(len(class_names))\n",
    "class_dict = dict(zip(class_names, lang_idxs))\n",
    "\n",
    "print(f\"There are {len(class_names)} classes.\")\n",
    "print(f\"There are {len(class_dict)} classes.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset using Yamnet model as feature-extractor\n",
    "\n",
    "Yamnet requires a slightly different way of preprocessing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model = tf.saved_model.load('./yamnet_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_wav = path_to_audio('data/audio_wav/afrikaans1.wav')\n",
    "\n",
    "def load_16k_audio_wav(filename):\n",
    "    # Read file content\n",
    "    file_content = tf.io.read_file(filename)\n",
    "\n",
    "    # Decode audio wave\n",
    "    audio_wav, sample_rate = tf.audio.decode_wav(file_content, desired_channels=1)\n",
    "    audio_wav = tf.squeeze(audio_wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "\n",
    "    # Resample to 16k\n",
    "    audio_wav = tfio.audio.resample(audio_wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return audio_wav\n",
    "\n",
    "\n",
    "def filepath_to_embeddings(filename, label):\n",
    "    # Load 16k audio wave\n",
    "    audio_wav = load_16k_audio_wav(filename)\n",
    "\n",
    "    # Get audio embeddings & scores.\n",
    "    # The embeddings are the audio features extracted using transfer learning\n",
    "    # while scores will be used to identify time slots that are not speech\n",
    "    # which will then be gathered into a specific new category 'other'\n",
    "    scores, embeddings, _ = yamnet_model(audio_wav)\n",
    "\n",
    "    # Number of embeddings in order to know how many times to repeat the label\n",
    "    embeddings_num = tf.shape(embeddings)[0]\n",
    "    labels = tf.repeat(label, embeddings_num)\n",
    "\n",
    "    # Change labels for time-slots that are not speech into a new category 'other'\n",
    "    # labels = tf.where(tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1)\n",
    "\n",
    "    # Using one-hot in order to use AUC\n",
    "    return (embeddings, tf.one_hot(labels, len(class_names)))\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(dataframe, batch_size=64):\n",
    "    # print(dataframe.info())\n",
    "    # Rewrite file_name to contain file paths\n",
    "    dataframe['file_name'] = dataframe.apply(\n",
    "        lambda row: os.path.join(os.getcwd(), 'data/audio_wav', row[\"file_name\"] + \".wav\"), \n",
    "        axis=1\n",
    "    )\n",
    "    # Convert the labels into numbers\n",
    "    dataframe['native_langs'] = dataframe.apply(\n",
    "        lambda row: class_dict[row['native_langs']],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"file_name\"], dataframe[\"native_langs\"]))\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: filepath_to_embeddings(x, y),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    ).unbatch()\n",
    "\n",
    "    return dataset.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 886\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(os.getcwd() + \"/data/audio.csv\", on_bad_lines='skip', delimiter=';')\n",
    "dataframe = dataframe[(dataframe['native_langs'] == 'english') | (dataframe['native_langs'] == 'spanish')]\n",
    "print(f'Number of data points: {len(dataframe)}')\n",
    "\n",
    "# Splitting training and validation set\n",
    "split = int(len(dataframe) * 0.8)\n",
    "train_df = dataframe[:split]\n",
    "valid_df = dataframe[split:]\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_df)\n",
    "valid_ds = dataframe_to_dataset(valid_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(hp):\n",
    "    inputs = keras.layers.Input(shape=(1024), name=\"embedding\")\n",
    "\n",
    "    # Tune the number of units in the first Dense layer. Choose an optimal value between 32-512\n",
    "    hp_units = hp.Int('dense_layer_1', min_value=32, max_value=512, step=32)\n",
    "    x = keras.layers.Dense(units=hp_units, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = keras.layers.Dropout(0.15, name=\"dropout_1\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    x = keras.layers.Dropout(0.2, name=\"dropout_2\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(192, activation=\"relu\", name=\"dense_3\")(x)\n",
    "    x = keras.layers.Dropout(0.25, name=\"dropout_3\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_4\")(x)\n",
    "    x = keras.layers.Dropout(0.2, name=\"dropout_4\")(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", name=\"ouput\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"accent_recognition\")\n",
    "\n",
    "    # Tune the learning rate for the optimizer. Choose an optimal value from 0.001, 0.0001, or 1.9644e-5\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1.9644e-5])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_and_compile_model,\n",
    "                    objective='val_accuracy',\n",
    "                    max_epochs=20,\n",
    "                    factor=3,\n",
    "                    hyperband_iterations=10,\n",
    "                    directory='uu_accent_detection_dir',\n",
    "                    project_name='uu_accent_detection',\n",
    "                    overwrite=False)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(train_ds, \n",
    "            epochs=20, \n",
    "            validation_data=valid_ds, \n",
    "            callbacks=[stop_early],\n",
    "            verbose=2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. \n",
    "The optimal number of units in the first densely-connected layers is {best_hps.get('dense_layer_1')}. \n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class weight calculation\n",
    "\n",
    "Since the dataset is quite unbalanced, we wil use class_weight argument during training.\n",
    "\n",
    "Getting the class weights is a little tricky because even though we know the number of audio files for each class, it does not represent the number of samples for that class since Yamnet transforms each audio file into multiple audio samples of 0.96 seconds each. So every audio file will be split into a number of samples that is proportional to its length.\n",
    "\n",
    "Therefore, to get those weights, we have to calculate the number of samples for each class after preprocessing through Yamnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = tf.zeros(shape=(len(class_names),), dtype=tf.int32)\n",
    "\n",
    "for x, y in iter(train_ds):\n",
    "    class_counts = class_counts + tf.math.bincount(\n",
    "        tf.cast(tf.math.argmax(y, axis=1), tf.int32), minlength=len(class_names)\n",
    "    )\n",
    "\n",
    "class_weight = {\n",
    "    i: tf.math.reduce_sum(class_counts).numpy() / class_counts[i].numpy()\n",
    "    for i in range(len(class_counts))\n",
    "}\n",
    "\n",
    "print(class_weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Keras callbacks in order to:\n",
    "\n",
    "- Stop whenever the validation AUC stops improving.\n",
    "- Save the best model.\n",
    "- Call TensorBoard in order to later view the training and validation logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=10, restore_best_weights=True)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(MODEL_NAME + \".h5\", monitor=\"val_auc\", save_best_only=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(os.path.join(os.curdir, \"logs\", hypermodel.name))\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "history = hypermodel.fit(train_ds, \n",
    "                        validation_data=valid_ds,\n",
    "                        epochs=EPOCHS,\n",
    "                        class_weight=class_weight,\n",
    "                        batch_size=kt.HyperParameters.Choice('batch_size', [16, 32]),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=2)\n",
    "\n",
    "# val_acc_per_epoch = history.history['val_accuracy']\n",
    "# best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "# print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, train_auc = hypermodel.evaluate(train_ds)\n",
    "valid_loss, valid_acc, valid_auc = hypermodel.evaluate(valid_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Plotting the accuracy and AUC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "axs[0].plot(range(EPOCHS), history.history[\"accuracy\"], label=\"Training\")\n",
    "axs[0].plot(range(EPOCHS), history.history[\"val_accuracy\"], label=\"Validation\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_title(\"Training & Validation Accuracy\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(range(EPOCHS), history.history[\"auc\"], label=\"Training\")\n",
    "axs[1].plot(range(EPOCHS), history.history[\"val_auc\"], label=\"Validation\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_title(\"Training & Validation AUC\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Let's now plot the confusion matrix for the validation dataset.\n",
    "\n",
    "The confusion matrix lets us see, for every class, not only how many samples were correctly classified, but also which other classes were the samples confused with.\n",
    "\n",
    "It allows us to calculate the precision and recall for every class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y tensors\n",
    "x_valid = None\n",
    "y_valid = None\n",
    "\n",
    "for x, y in iter(valid_ds):\n",
    "    if x_valid is None:\n",
    "        x_valid = x.numpy()\n",
    "        y_valid = y.numpy()\n",
    "    else:\n",
    "        x_valid = np.concatenate((x_valid, x.numpy()), axis=0)\n",
    "        y_valid = np.concatenate((y_valid, y.numpy()), axis=0)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = hypermodel.predict(x_valid)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(\n",
    "    np.argmax(y_valid, axis=1), np.argmax(y_pred, axis=1)\n",
    ")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    confusion_mtx, xticklabels=class_names, yticklabels=class_names, annot=True, fmt=\"g\"\n",
    ")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision & Recall\n",
    "\n",
    "For every class:\n",
    "\n",
    "- Recall is the ratio of correctly classified samples i.e. it shows how many samples of this specific class, the model is able to detect. It is the ratio of diagonal elements to the sum of all elements in the row.\n",
    "- Precision shows the accuracy of the classifier. It is the ratio of correctly predicted samples among the ones classified as belonging to this class. It is the ratio of diagonal elements to the sum of all elements in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(class_names):\n",
    "    precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])\n",
    "    recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])\n",
    "    print(\n",
    "        \"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(\n",
    "            label, precision * 100, recall * 100\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
