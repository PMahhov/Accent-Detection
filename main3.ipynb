{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accent-recognizing model, same as in main2.ipynb\n",
    "Including hyperparam tuning.\n",
    "\n",
    "Not included:\n",
    "- Yamnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q tensorflow tensorflow_datasets\n",
    "#apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "%pip install -U -q keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "2023-01-17 21:23:42.574143: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from os.path import isfile, join, splitext\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_hub as tfhub\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "import keras_tuner as kt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_SEED = 43\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many classes are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 228 classes.\n",
      "There are 228 classes.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/data/audio.csv\", on_bad_lines = 'skip' , delimiter= ';')\n",
    "# print(len(df['native_langs'].unique()))\n",
    "# for lang in df['native_langs'].unique():\n",
    "#     print(f'\"{lang}\",')\n",
    "class_names = df['native_langs'].unique()\n",
    "lang_idxs = range(len(class_names))\n",
    "class_dict = dict(zip(class_names, lang_idxs))\n",
    "\n",
    "print(f\"There are {len(class_names)} classes.\")\n",
    "print(f\"There are {len(class_dict)} classes.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_audio(path):\n",
    "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "    return audio\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    \n",
    "    # print(dataframe.info())\n",
    "    # Rewrite file_name to contain file paths\n",
    "    dataframe['file_name'] = dataframe.apply(\n",
    "        lambda row: os.path.join(os.getcwd(), 'data/audio_wav', row[\"file_name\"] + \".wav\"), \n",
    "        axis=1\n",
    "    )\n",
    "    # Convert the labels into numbers\n",
    "    dataframe['native_langs'] = dataframe.apply(\n",
    "        lambda row: class_dict[row['native_langs']],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(dataframe['file_name'])\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(dataframe['native_langs'])\n",
    "    \n",
    "    # print(len(path_ds), len(audio_ds), len(label_ds))\n",
    "    # dataframe = df[(df[\"native_langs\"] == \"amharic\") | (df[\"native_langs\"] == \"indonesian\")]\n",
    "    # print(dataframe.shape)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def audio_to_fft(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension, we need to squeeze the dimensions and then expand them again after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the list of audio file paths and labels from the CSV file, as Pandas dataframe.\n",
    "\n",
    "**IMPORTANT!**\n",
    "To avoid noises, we are experimenting with only the most frequent classes in our dataset (English and Spanish). In order to work with the entire classes, we are going to identify infrequent classes and remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(os.getcwd() + \"/data/audio.csv\", on_bad_lines = 'skip' , delimiter= ';')\n",
    "dataframe = dataframe[(dataframe['native_langs'] == 'english') | (dataframe['native_langs'] == 'spanish')]\n",
    "print(len(dataframe))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training & validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 21:24:12.479185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "# rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "# rng.shuffle(audio_paths)\n",
    "# rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "# rng.shuffle(labels)\n",
    "\n",
    "# Splitting training and validation set\n",
    "split = int(len(dataframe) * 0.8)\n",
    "train_df = dataframe[:split]\n",
    "valid_df = dataframe[split:]\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_df)\n",
    "valid_ds = dataframe_to_dataset(valid_df)\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(BATCH_SIZE)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam tuning\n",
    "\n",
    "Reference https://www.tensorflow.org/tutorials/keras/keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
    "    # Shortcut\n",
    "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
    "    for i in range(conv_num - 1):\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "    inputs = keras.layers.Input(shape=(SAMPLING_RATE // 2, 1), name=\"input\")\n",
    "\n",
    "    x = residual_block(inputs, 16, 2)\n",
    "    x = residual_block(x, 32, 2)\n",
    "    x = residual_block(x, 64, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "\n",
    "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "\n",
    "    # Tune the number of units in the first and second Dense layer. Choose an optimal value between 32-512\n",
    "    hp_units = hp.Int('dense_layer_1', min_value=32, max_value=512, step=32)\n",
    "    hp_units2 = hp.Int('dense_layer_2', min_value=32, max_value=512, step=32)\n",
    "    x = keras.layers.Dense(units=hp_units, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(units=hp_units2, activation=\"relu\")(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Tune the learning rate for the optimizer. Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), \n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the hyperparameter tuner. \n",
    "# Set overwrite=False to continue the search from last session, overwrite=True to start new search everytime.\n",
    " \n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                    objective='val_accuracy',\n",
    "                    max_epochs=20,\n",
    "                    factor=3,\n",
    "                    hyperband_iterations=10,\n",
    "                    directory='uu_accent_detection_dir',\n",
    "                    project_name='uu_accent_detection',\n",
    "                    overwrite=False)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(train_ds, \n",
    "            epochs=20, \n",
    "            validation_data=valid_ds, \n",
    "            callbacks=[stop_early],\n",
    "            verbose=2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. \n",
    "The optimal number of units in the first and second densely-connected layers \n",
    "are {best_hps.get('dense_layer_1')} and {best_hps.get('dense_layer_2')}. \n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to stop training when the model is not enhancing anymore\n",
    "stop_early_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "history = hypermodel.fit(train_ds, \n",
    "                        validation_data=valid_ds,\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=kt.HyperParameters.Choice('batch_size', [16, 32]),\n",
    "                        callbacks=[stop_early_cb])\n",
    "\n",
    "# val_acc_per_epoch = history.history['val_accuracy']\n",
    "# best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "# print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the hypermodel on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = hypermodel.evaluate(valid_ds)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Georgios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(hp):\n",
    "#     \"\"\"\n",
    "#     Builds model and sets up hyperparameter space to search.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     hp : HyperParameter object\n",
    "#         Configures hyperparameters to tune.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     model : keras model\n",
    "#         Compiled model with hyperparameters to tune.\n",
    "#     \"\"\"\n",
    "#     # Initialize sequential API and start building model.\n",
    "#     num_classes = len(y_train[0])\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(hp.Int('filters_1', min_value=32, max_value=256, step=32), (3, 3), activation='relu',\n",
    "#                      input_shape=(X_train.shape[1], X_train.shape[2], 1),padding='same'))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     for i in range(1, hp.Int(\"num_layers\", 2, 6)):\n",
    "#         model.add(\n",
    "#             keras.layers.Conv2D(hp.Int('filters_'+str(i), min_value=32, max_value=256, step=32), (3, 3), activation='relu', padding='same')\n",
    "#         )\n",
    "#         model.add(keras.layers.MaxPooling2D(pool_size=(2, 2),padding='same', data_format='channels_last'))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     for i in range(1, hp.Int(\"num_layers\", 2, 6)):\n",
    "#         model.add(\n",
    "#             keras.layers.Dense(\n",
    "#                 units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "#                 activation=\"relu\")\n",
    "#         )\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# hp = HyperParameters()\n",
    "\n",
    "# tuner = Hyperband(build_model,\n",
    "#                 objective=\"val_accuracy\",\n",
    "#                 max_epochs=20,\n",
    "#                 factor=3,\n",
    "#                 hyperband_iterations=10,\n",
    "#                 directory=\"kt_dir\",\n",
    "#                 project_name=\"kt_hyperband\")\n",
    "\n",
    "# stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# tuner.search(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[stop_early], verbose=2)\n",
    "# best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# #### Build model\n",
    "\n",
    "# h_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# #### Train the hypertuned model\n",
    "\n",
    "# h_model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[stop_early],batch_size=hp.Choice(\"batch_size\", [16, 32]), verbose=2)\n",
    "# y_pred = h_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (v3.10.7:6cc6b13308, Sep  5 2022, 14:02:52) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
